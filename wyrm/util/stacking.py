"""
:module: wyrm.util.stacking
:auth: Nathan T. Stevens
:email: ntsteven (at) uw.edu
:org: Pacific Northwest Seismic Network
:license: AGPL-3.0

:purpose:
    This module contains subroutines built on numpy that facilitate overlapping
    array stacking and buffering, largely used by the RtPredBuff class, and
    coherence/semblance methods based on code from the ELEP project (Yuan et al., 2023)

:references:
Yuan, C, Ni, Y, Lin, Y, Denolle, M, 2023, Better Together: Ensemble Learning for Earthquake
    Detection and Phase Picking. IEEE Transactions on Geoscience and Remote Sensing, 61,
    doi: 10.1109/TGRS.2023.3320148

"""

import numpy as np
import scipy.ndimage as nd
from itertools import chain, combinations

def powerset(iterable, with_null=False):
    """
    Create a powerset from an iterable object comprising set elements

    scales as 2**len(iterable) (including the null set)
    
    {1, 2, 3} -> [(1,2,3), 
                  (1,2), (1,3), (2,3),
                  (1), (2), (3),
                  ()] <-- NOTE: null setexcluded if with_null=False

    :: INPUTS ::
    :param iterable: [lis] iterable set of elements from which
                        a power set is generated.
                    NOTE: iterable is passed through a list(set(iterable))
                        nest to ensure no duplicate entries
    :param with_null: [bool] should the null set be included

    :: OUTPUT ::
    :return out: [list] of [tuple] power set
    
    Source Attribution: User Mark Rushakoff and edits by User Ran Feldesh
    https://stackoverflow.com/questions/1482308/how-to-get-all-subsets-of-a-set-powerset
    """
    s = list(set(iterable))
    if with_null:
        out = chain.from_iterable(combinations(s, r) for r in range(len(s)+1))
    else:
        out = chain.from_iterable(combinations(s, r) for r in range(1, len(s)+1))
    return out

def shift_trim(array, npts_right, axis=None, fill_value=np.nan, dtype=None, **kwargs):
    """
    Execute a shift of values along a specified axis (default 0-axis),
    omitting (trimming) samples that fall out of the reference frame of the
    array and padding unoccupied sample positions with a specified `fill_value`

    Operation generates a copy of input `array` data, with modifications

    :: INPUTS ::
    :param array: [numpy.ndarray] target array to shift
    :param npts_right: [int] number of samples to shift contents, with a
                positive value indicating a rightward (increasing index)
                shift and a negative value indicating a leftward (decreasing
                index) shift
    :param axis: [int] axis over which to enact the leftshift
    :param fill_value: [scalar] fill_value to insert into empty index positions
                generated by shifting and trimming

    Benchmarking shows that the index-reassignment approach done here
    is more computationally efficient compared to roll_trim_rows below by 
    about a factor of 3

    """
    if not isinstance(array, np.ndarray):
        raise TypeError('input "array" must be type numpy.ndarray')
    
    if not isinstance(npts_right, (int, float)):
        raise TypeError('npts_right must be int-like')
    elif isinstance(npts_right, float):
        npts_right = int(npts_right)
    
    if axis is None:
        pass
    elif axis + 1 > len(array.shape):
        raise IndexError(f'Axis {axis} is outside the dimensionality of array ({array.shape})')

    
    # End processing and return 
    if npts_right == 0:
        return array.copy()
    else:
        if dtype is None:
            tmp_array = np.full(shape=array.shape, fill_value=fill_value, dtype=array.dtype, **kwargs)
        else:
            tmp_array = np.full(shape=array.shape, fill_value=fill_value, **kwargs)

        # If ellipsis use case
        if axis is None:
            # rightshift
            if npts_right > 0:
                tmp_array[npts_right:, ...] = array.copy()[:-npts_right, ...]
            # leftshift
            elif npts_right < 0:
                tmp_array[:npts_right, ...] = array.copy()[-npts_right:, ...]
            
        elif axis + 1 == array.ndim or axis == -1:
            # rightshift
            if npts_right > 0:
                tmp_array[..., npts_right:] = array.copy()[..., :-npts_right]
            # leftshift
            elif npts_right < 0:
                tmp_array[..., :npts_right] = array.copy()[..., -npts_right:]
        # hack-ey use of 'eval' to do dynamic indexing
        elif axis + 1 < array.ndim:
            # Start eval strings for indexing
            tmp_index = '['
            array_index = '['
            for _i in range(array.ndim):
                # If this is not the target axis, insert :
                if _i != axis:
                    tmp_index += ':'
                    array_index += ':'
                # If this is the target axis
                else:
                    # append rightshift slice
                    if npts_right > 0:
                        tmp_index += 'npts_right:'
                        array_index += ':-npts_right'
                    # append lefshift slice
                    elif npts_right < 0:
                        tmp_index += ':-npts_right'
                        array_index == 'npts_right:'
                # If this isn't the final axis, append a comma
                if _i + 1 < len(array.shape):
                    tmp_index += ', '
                    array_index += ', '
                # Otherwise, close out eval str
                else:
                    tmp_index += ']'
                    array_index += ']'
            eval(f'tmp_array{tmp_index} = array.copy(){array_index}')
    
    return tmp_array





def neighbor_sum(arr):
    '''
    DIRECT COPY FROM: 
    https://github.com/congcy/ELEP/blob/main/ELEP/elep/ensemble_coherence.py
    '''
    return sum(arr)

def ensemble_semblance(signals, paras):
    '''
    DIRECT COPY FROM: 
    https://github.com/congcy/ELEP/blob/main/ELEP/elep/ensemble_coherence.py

    Function: calculate coherence or continuity via semblance analysis.
    Reference: Marfurt et al. 1998
    
    PARAMETERS:
    ---------------
    signals: input data [ntraces, npts]
    paras: a dict contains various parameters used for calculating semblance. 
           Note: see details in Tutorials.
    
    RETURNS:
    ---------------
    semblance: derived cohence vector [npts,]
    
    Written by Congcong Yuan (Jan 05, 2023)
    '''
    # setup parameters
    ntr, npts = signals.shape 
    dt = paras['dt']
    semblance_order = paras['semblance_order']
    semblance_win = paras['semblance_win']
    weight_flag = paras['weight_flag']
    window_flag = paras['window_flag']
    
    semblance_nsmps = int(semblance_win/dt)
    
    # initializing
    semblance, v = np.zeros(npts), np.zeros(npts)
    
    # sums over traces
    square_sums = np.sum(signals, axis=0)**2
    sum_squares = np.sum(signals**2, axis=0)
    
    # loop over all time points
    if weight_flag:
        if weight_flag == 'max':
            v = np.amax(signals, axis=0)
        elif weight_flag == 'mean':
            v = np.mean(signals, axis=0)
        elif weight_flag == 'mean_std':
            v_mean = np.mean(signals, axis=0)
            v_std = np.mean(signals, axis=0)
            v = v_mean/v_std
    else:
        v = 1.
        
    if window_flag:
        # sum over time window
        sums_num = nd.generic_filter(square_sums, neighbor_sum, semblance_nsmps, mode='constant')
        sums_den = ntr*nd.generic_filter(sum_squares, neighbor_sum, semblance_nsmps, mode='constant')
    else:
        sums_num = square_sums
        sums_den = ntr*sum_squares

    # original semblance
    semblance0 = sums_num/sums_den

    # enhanced semblance
    semblance = semblance0**semblance_order*v 
    
    return semblance  

def preprocess_semblance_weights(signals, weights=None, enhanced_wt='max'):
    if weights is None:
        dwt = np.ones(shape=signals.shape, dtype=signals.dtype)
    elif signals.shape != weights.shape:
        raise IndexError(f'shapes of signals and weights mismatch')
    elif weights.dtype != signals.dtype:
        dwt = weights.astype(signals.dtype)
    
    if enhanced_wt == 'max':
        ewt = np.amax(signals, axis=0)
    elif enhanced_wt == 'mean':
        ewt = np.mean(signals, axis=0)
    elif enhanced_wt == 'mean_std':
        ewt = np.mean(signals, axis=0)/np.std(signals, axis=0)
    else:
        ewt = np.ones(shape=signals.shape[-1])
    
    out = {'data_weights': dwt,
           'enhancement_weights': ewt}
    return out
        

# def powerset_ensemble_semblance(square_sums, summed_squares, data_weights, enhancement)


def semblance(pstack, fstack=None, order=2, semb_npts=101, method='np', eta=1e-9, weighted=True):
    """
    Calculate coherence via semblance analysis

    Based on ELEP's ensemble_coherence method
    Yuan et al. (2023) and references therein

    Semblance:
    semb(t) = MAX[m](P(m,t))*C[m,t](P(m,t))**order

    Coherence:
    C(P(t)) = sum[ti, tj]{sum[m]{P(m,t)**2}}/
                M*sum[ti,tj]{sum[m]{P(m,t)**2}}

    With M = the number of entries in "m"

    :: INPUTS ::
    :param pstack: [numpy.ndarray] containing stacked, synchronous windowed
                    prediction outputs from "m" model-weight combinations
                    for a given prediction label "l" time-series with sample
                    points "t".
                    This version requires the following axis assignment
                        pstack [m, l, t]
                            or
                        pstack [m, t]
                        when run in method = 'nd'
                    or generally
                        pstack [m, ..., t]
                        when run in method = 'np'
 
                    NOTE
                    If mixing model types (i.e., EQTransformer & PhaseNet)
                    care is required to make sure l-axis inputs are compatable
                    i.e., use prediction labels 1 (P-detection) and 2 (S-detection)
                        but likely omit label 0 (Detection for EqT, Noies for PhaseNet)

    :param order: [int] exponent to apply to the semblance term
    :param semb_npts: [int] number of samples in the window [ti, tj]
            NOTE: computational cost scales roughly linearly with semb_npts
    :param method: [str] method flag for the algorithm to produce rolling windows
                of stack along the t_axis.
            Supported:
                "nd" - use SciPy's ndimage.generic_filter() method (after Yuan et al., 2023)
                        Slightly less computationally efficient, but provides estimates
                        at the edges of arrays.
                "np" - use NumPy's lib.stride_tricks.sliding_window_view() method
                        Slightly more computationally efficient, but blinds the
                        first and last semb_npts/2 points in each row vector of semb_array
                        (blind = np.nan)
    :param eta: [float] very small floating point number added to the denominator of
                C(P(t)) to prevent division by 0. Default is 1e-9. 
    :param weighted: [bool] should the MAX weighting term be applied?
                        True - weighting is via MAX (weighted semblance metric)
                        False - weighting is uniformly 1 (original semblance metric)
                        
    :: OUTPUT ::
    :return semb_array: [numpy.ndarray] containing semblance time-series with dimensionality
                semb_array [l, t] for input pstack [m, l, t]
                    or
                semb_array [t] for input pstack [m, t]
    """
    # TODO - compatability checks
    if eta < 0:
        eta = np.abs(eta)

    if weighted:
        # Calculate max weighting for each P(l,t)
        wt = np.max(pstack,axis=0)
    else:
        wt = np.ones(shape=pstack.shape[1:])

    if method == 'np':
        # Preallocate output array
        semb_array = np.full(shape=pstack.shape[1:],
                            fill_value=np.nan,
                            dtype=pstack.dtype)
        # Get first write location
        first_t = semb_npts//2
        # Generate views
        views = np.lib.stride_tricks.sliding_window_view(pstack, semb_npts, axis=-1)
        # Calculate numerator
        num = np.sum(np.sum(views, axis=0)**2, axis=-1)
        # Calculate denominator
        den = pstack.shape[0]*np.sum(views**2, axis=(0,-1))
        semb_array[..., first_t: first_t + views.shape[-2]] = (num/(den+eta))**order
        semb_array *= wt

    if method == 'nd':
        # Calculate internal sum/square combinations
        sq_sums = np.sum(pstack, axis=0)**2
        sum_sqs = np.sum(pstack**2, axis=0)
        # Iterate across labels
        if pstack.ndim == 3:
            for _l in range(pstack.shape[1]):
                # Apply 
                num = nd.generic_filter(sq_sums[_l, :], sum, semb_npts, mode='constant')
                den = pstack.shape[0]*nd.generic_filter(sum_sqs[_l, :], sum, semb_npts, mode='constant')
                semb_array[_l, :] = wt[_l, :]*(num/(den+eta))**order
        # Handle case where label axis has been squeezed
        elif pstack.ndim == 2:
            num = nd.generic_filter(sq_sums, sum, semb_npts, mode='constant')
            den = pstack.shape[0]*nd.generic_filter(sum_sqs, sum, semb_npts, mode='constant')
            semb_array = wt*(num/(den+eta))**order


    return semb_array

# def fold_weighted_semblance(
#         pstack,
#         fstack=None,
#         semb_order=2,
#         max_scaled=True,
#         semb_npts=101,
#         method='np',
#         eta=1e-9):
#     """
#     Use a matching dimensionality fold array for predictions to calculate a 


#              sum(t=ti - dt to ti + dt){ sum(m=0 to M-1) {P_m(t)*f_m(t)}**2}
#     Sw(ti) = --------------------------------------------------------------
#              sum(t=ti - dt to ti + dt){(sum(m=0 to M-1){P_m(t)**2} sum(m=0 to M-1){f_m(t)**2}}

#     https://reproducibility.org/RSF/book/tccs/vscan/paper_html/node4.html
#     """
#     # Parse fold weighting array
#     if fstack is None:
#         fstack = np.ones(pstack.shape, dtype=pstack.dtype)
#         ftype = 'ones'
#     else:
#         ftype = 'data'
#     if pstack.shape != fstack.shape:
#         raise ValueError(f'pstack and fstack shape do not match')
#     # Get maximum scalar vector
#     if max_scaled:
#         max_scalar = np.max(pstack, axis=0)
#     else:
#         max_scalar = np.ones(pstack.shape[1])

#     if method in ['numpy', 'np']:
#         semb_array = np.full(shape=pstack.shape[1:],
#                              fill_value=np.nan,
#                              dtype=pstack.dtype)
#         first_t = semb_npts//2
#         # Generate sliding window views of data and fold
#         pviews = np.lib.stride_tricks.sliding_window_view(pstack, semb_npts, axis=-1)
#         fviews = np.lib.stride_tricks.sliding_window_view(fstack, semb_npts, axis=-1)

#         M = pstack.shape[0]
#         num = np.sum(np.sum(pviews*fviews, axis=0)**2, axis=-1)
#         den = np.sum(np.sum(pviews, axis=0)**2 * np.sum(fviews, axis=0)**2, axis=-1)
#         semb_array[..., first_t: first_t + pviews.shape[-2]] = num / (den + eta)
                
#         c2 = np.where(semb_array > 0, semb_array**2, 0)
#         c2 *=max_scalar



def generate_example_semblance_data(nlabels=1,mmodels=30):
    # Create holder
    pstack = np.zeros(shape=(mmodels, nlabels, 3000))
    # Create example vector
    vect = np.zeros(3000)
    # Create a bell-curvey probability distribution
    vect[1000:1009] = np.array([0.01,0.03,0.1,0.3,1,0.3,0.1,0.03,0.01])
    # Iterate across model and label indices
    for _m in range(mmodels):
        for _l in range(nlabels):
            # Perturb example vector with roll and add to pstack
            pstack[_m, _l, :] = np.roll(vect.copy(), (_m+_l)%4 + _l)
    return pstack