"""
:module: wyrm.core.process
:auth: Nathan T. Stevens
:email: ntsteven (at) uw.edu
:org: Pacific Northwest Seismic Network
:license: AGPL-3.0

:purpose:
    This module hosts class definitions for Wyrm submodules that process data:

        WindowWyrm - a submodule for sampling a waveform buffer and generating forward-marching
                    windowed copies of these data as they become available
                        PULSE
                            input: DictStream holding MLTraceBuffer objects
                            output: deque holding ComponentStream objects
        MethodWyrm - a submodule for applying a single class method to objects presented to the
                    MethodWyrm
                        PULSE
                            input: deque of objects
                            output: deque of objects
        PredictWyrm - a submodule for runing predictions with a particular PyTorch/SeisBench model
                    architecture with pretrained weight(s) on preprocessed waveform data
                        PULSE:
                            input: deque of preprocessed ComponentStream objects
                            output: deque of MLTrace objects
"""
import torch, time, copy
import seisbench.models as sbm
import numpy as np
from collections import deque
from obspy import UTCDateTime
from wyrm.data.mltrace import MLTrace
from wyrm.data.mltracebuffer import MLTraceBuffer
from wyrm.core._base import Wyrm
from wyrm.data.dictstream import DictStream
from wyrm.data.componentstream import ComponentStream
from wyrm.util.compatability import bounded_floatlike, bounded_intlike


class WindowWyrm(Wyrm):
    """
    The WindowWyrm class takes windowing information from an input
    seisbench.models.WaveformModel object and user-defined component
    mapping and data completeness metrics and provides a pulse method
    that iterates across entries in an input DictStream object and
    generates ComponentStream copies of sampled data that pass data completeness
    requirements. These ComponentStreams are then appended to the WindowWyrm's
    queue attribute [a collections.deque object] via an 'append' action
    """

    def __init__(
        self,
        component_aliases={"Z": "Z3", "N": "N1", "E": "E2"},
        reference_component='Z',
        reference_completeness_threshold=0.95,
        model_name="EQTransformer",
        reference_sampling_rate=100.0,
        reference_npts=6000,
        reference_overlap=1800,
        pulse_type='network',
        max_pulse_size=1,
        debug=False,
        **options
    ):
        """
        Initialize a WindowWyrm object that samples a DictStream of MLTraceBuffer
        objects and generates ComponentStream copies of windowed data if a reference
        component for a given instrument in the DictStream is present and has
        sufficient data to meet windowing requirements

        :: INPUTS ::
        :param ref_comp: [str] reference component code passed to initialization
                    of ComponentStream objects
        :param component_aliases: [dict] dictionary defining aliases for component
                    codes with keys representing the aliases and values representing
                    iterable strings of component names that correspond to each alias
        :param ref_comp_thresh: [float] fractional completeness threshold value for
                    the reference component when generating a given ComponentStream
        :param model_name: [str] model name to append to the MLTrace.stats in 
                    ComponentStreams generated by this WindowWyrm
        :param reference_sampling_rate: [float] sampling rate in samples per second
                    to which component streams are referenced. In conjunction with
                    the reference_npts, these parameters define window lengths in
                    seconds that are used to sample buffered data. This value is 
                    assigned to ComponentStream's stats.reference_sampling_rate 
                    attribute
        :param reference_npts: [int] number of samples that completely processed
          ComponentStreams' traces
                    must have. This value is assigned to ComponentStreams' 
                    stats.reference_npts attribute
        :param reference_overlap: [int] number of overlapping samples for
                    sequential windows. In conjunction with reference_sampling_rate
                    this dictates the number of seconds the window starttimes
                    are advanced for window generation
        :param max_pulse_size: [int] maximum number of sweeps across every entry
                    in an input DictStream to conduct to try to generate window
                    I.e., maximum number of windows to generate per instrument
                    represented in an input DictStream.
        :param debug: [bool] - run in debug mode?
        :param **options: [kwargs] key-word argument collector to pass to
                    MLTrace.trim() via MLTraceBuffer.trim_copy() methods


        """
        # Initialize/inherit from Wyrm
        super().__init__(max_pulse_size=max_pulse_size, debug=debug)

        if pulse_type.lower() in ['network','site','instrument']:
            self.pulse_type = pulse_type.lower()
            self.next_code = None
        else:
            raise ValueError(f'pulse_type "{pulse_type}" not supported.')

        # Compatability checks for component_aliases
        if not isinstance(component_aliases, dict):
            raise TypeError('component_aliases must be type dict')
        elif not all(isinstance(_v, str) and _k in _v for _k, _v in component_aliases.items()):
            raise SyntaxError('component_aliases values must be type str and include the key value')
        else:
            self.aliases = component_aliases

        # Compatability check for reference_component
        if reference_component in self.aliases.keys():
            refc = reference_component
        else:
            raise ValueError('reference_component does not appear as a key in component_aliases')
        
        # Compatability check for reference_completeness_threshold
        reft = bounded_floatlike(
            reference_completeness_threshold,
            name = "reference_completeness_threshold",
            minimum=0,
            maximum=1,
            inclusive=True
        )
        if not isinstance(model_name, str):
            raise TypeError('model_name must be type str')
        else:
            self.model_name = model_name

        # Target sampling rate compat check
        refsr = bounded_floatlike(
            reference_sampling_rate,
            name='reference_sampling_rate',
            minimum=0,
            maximum=None,
            inclusive=False
        )
        # Target window number of samples compat check
        refn = bounded_intlike(
            reference_npts,
            name='reference_npts',
            minimum=0,
            maximum=None,
            inclusive=False
        )
        # Target window number of sample overlap compat check
        refo = bounded_intlike(
            reference_overlap,
            name='reference_overlap',
            minimum=0,
            maximum=None,
            inclusive=False
        )
        self.ref = {'sampling_rate': refsr, 'overlap': refo, 'npts': refn, 'component': refc, 'threshold': reft}
         # Set Defaults and Derived Attributes
        # Calculate window length, window advance, and blinding size in seconds
        self.window_sec = (self.ref['npts'] - 1)/self.ref['sampling_rate']
        self.advance_npts = self.ref['npts'] - self.ref['overlap']
        self.advance_sec = (self.ref['npts'] - self.ref['overlap'])/self.ref['sampling_rate']

        self.options = options

        # Create dict for holding instrument window starttime values
        self.window_tracker = {}
        # Have futuredate as a standin for minimum starttime during assimilation
        # self.default_starttime = UTCDateTime('2366-12-31T23:59:59.999999')
        # Create queue for output collection of windows
        self.queue = deque()


    def pulse(self, x):
        """
        Primary method of WindowWyrm that executes up to max_pulse_size
        iterations of the following subroutines:
            _update_window_tracker
                --> Iterates across traces in `x` and updates the window_index
                    attribute with windowing parameters and assesses if windows
                    can be generated based on trace metadata
            _sample_windows
                --> Creates trimmed copies of traces in `x` housed in
                    ComponentStream objects, appending these to self.queue
        This pulse has an early stopping clause if no new windows are generated by
        _sample_windows.

        :: INPUT ::
        :param x: [wyrm.data.dictstream.DictStream] comprising MLTrace(Buffer) objects

        :: OUTPUT ::
        :return y: [collections.deque] access to this WindowWyrm's self.queue attribute
        """
        if not isinstance(x, DictStream):
            raise TypeError
        # Update window tracker with DictStream metadata
        self._update_window_tracker(x)
        # Run pulse based on pulse_type
        if self.pulse_type == 'network':
            if self.debug:
                print(f'∂∂∂ Window∂ - qlen {len(x)} - network pulse ∂∂∂')
            self._network_pulse(x)
        elif self.pulse_type == 'site':
            if self.debug:
                print(f'∂∂∂ Window∂ - qlen {len(x)} - site pulse ∂∂∂')
            self._sitewise_pulse(x)

        y = self.queue
        return y
        
    def _network_pulse(self, x):
        nnew = 0
        # Execute number of network-wide sweeps
        for _ in range(self.max_pulse_size):
            # Iterate across sites
            for site, _sv in self.window_tracker.items():
                if self.debug:
                    print(f'   {site}')
                # Iterate across instruments
                for inst, _ssv in _sv.items():
                    # Skip t0 reference
                    if isinstance(_ssv, dict):
                        # Iterate across model-weights
                        for mod in _ssv.keys():
                            nnew += self._sample_window(x, site, inst, mod)
            # If network sweep does not produce new windows, execute early stopping
            if nnew == 0:
                break



    def _sitewise_pulse(self, x):
        niter = 0
        # Get current list of sites
        sites = list(self.window_tracker.keys())
        # Iterate across site names
        for _i, site in enumerate(sites):
            # Handle very first site-wise looping catch on last_code
            if self.next_code is None:
                self.next_code = site
            
            if site == self.next_code:
                pass
            else:
                continue

            # if self.debug:
            #     print(f'    processing {site}')
            # If you're still here, iterate across instruments
            for inst, _ssv in self.window_tracker[site].items():
                # Skip over 't0' reference entry in window_tracker[site]
                if not isinstance(_ssv, dict):
                    continue
                for mod in _ssv.keys():
                    # Attempt to sample window (if approved)
                    nnew = self._sample_window(x, site, inst, mod)
                    # Get the site code of the next site in the list
                    if _i + 1 < len(sites):
                        self.next_code = sites[_i + 1]
                    # If we hit the end of the list, loop over to the beginning
                    else:
                        self.next_code = sites[0]
                    # if self.debug:
                    #     print(f'   next_code is {self.next_code}')
            # Increase the iteration counter
            niter += 1
            if niter > self.max_pulse_size:
                return
                        

    

    def _update_window_tracker(self, dst):
        """
        PRIVATE METHOD

        Scan across MLTraces in an input Dictionary Stream 
        with a component code matching the self.ref['component']
        attribute of this WindowWyrm and populate new branches in
        the self.window_index attribute if new site.inst
        (Network.Station.Location.BandInstrument{ref}) MLTraces are found

        :: INPUT ::
        :param dst: [wyrm.data.dictstream.DictStream] containing 
                        wyrm.data.mltrace.MLTrace type objects
        """
        for mltr in dst.traces.values():
            if not isinstance(mltr, MLTrace):
                raise TypeError('this build of WindowWyrm only works with wyrm.data.mltrace.MLTrace objects')
            # Get site, instrument, mod, and component codes from MLTrace
            site = mltr.site
            inst = mltr.inst
            comp = mltr.comp
            mod = mltr.mod
            ### WINDOW TRACKER NEW BRANCH SECTION ###
            # First, check if this is a reference component
            if comp not in self.ref['component']:
                # If not a match, continue to next trace
                continue
            # Otherwise proceed
            else:
                pass
            
            # If new site in window_tracker
            if site not in self.window_tracker.keys():
                # Populate t0's
                self.window_tracker.update({site: 
                                                {inst: 
                                                    {mod: 
                                                        {'ti': mltr.stats.starttime,
                                                         'ref': mltr.id,
                                                         'ready': False}},
                                                't0': mltr.stats.starttime}})
            # If site is in window_tracker
            else:
                # If inst is not in this site subdictionary
                if inst not in self.window_tracker[site].keys():
                    self.window_tracker[site].update({inst:
                                                        {mod:
                                                            {'ti': self.window_tracker[site]['t0'],
                                                             'ref': mltr.id,
                                                             'ready': False}}})
                # If inst is in this site subdictionary
                else:
                    # If mod is not in this inst sub-subdictionary
                    if mod not in self.window_tracker[site][inst].keys():
                        self.window_tracker[site][inst].update({mod:
                                                                    {'ti': self.window_tracker[site]['t0'],
                                                                     'ref': mltr.id,
                                                                     'ready': False}})
                        
            ### WINDOW TRACKER TIME INDEXING/WINDOWING STATUS CHECK SECTION ###   
            # Get window edge times
            next_window_ti = self.window_tracker[site][inst][mod]['ti']
            next_window_tf = next_window_ti + self.window_sec
            # If the trace has reached or exceeded the endpoint of the next window
            if next_window_tf <= mltr.stats.endtime:
                # Get valid fraction for proposed window in MLTrace
                fv = mltr.get_fvalid_subset(starttime=next_window_ti, endtime=next_window_tf)
                # If threshold passes
                if fv >= self.ref['threshold']:
                    # set (window) ready flag to True
                    self.window_tracker[site][inst][mod].update({'ready': True})
                    # And continue to next trace
                    continue
                # If threshold fails
                else:
                    # If the window was previously approved, recind approval
                    if self.window_tracker[site][inst][mod]['ready']:
                        self.window_tracker[site][inst][mod].update({'ready': False})

                    # If data start after the proposed window, increment window index to catch up
                    if next_window_ti < mltr.stats.starttime:
                        # Determine the number of advances that should be applied to catch up
                        nadv = 1 + (mltr.stats.starttime - next_window_ti)//self.advance_sec
                        # Apply advance
                        self.window_tracker[site][inst][mod]['ti'] += nadv*self.advance_sec
                        next_window_ti += nadv*self.advance_sec
                        next_window_tf += nadv*self.advance_sec
                        # If the new window ends inside the current data
                        if mltr.stats.endtime >= next_window_tf:
                            # Consider re-approving the window for copying + trimming
                            fv = mltr.get_fvalid_subset(starttime=next_window_ti,
                                                        endtime=next_window_tf)
                            # If window passes threshold, re-approve
                            if fv >= self.ref['threshold']:
                                self.window_tracker[site][inst][mod].update({'ready': True})
                        # Otherwise preserve dis-approval of window generation (ready = False) for now
                                

    def _sample_window(self, dst, site, inst, mod):
        """
        PRIVATE METHOD
        """
        nnew = 0
        _ssv = self.window_tracker[site][inst][mod]
        if _ssv['ready']:
            next_window_ti = _ssv['ti']
            next_window_tf = next_window_ti + self.window_sec
            # Subset data view
            _dst = dst.fnselect(f'{site}.{inst}?.{mod}')
            # Copy/trim traces from view
            traces = []
            for _mltb in _dst.traces.values():
                mlt = _mltb.trimmed_copy(starttime=next_window_ti,
                                        endtime=next_window_tf,
                                        pad=True,
                                        fill_value=None)
                mlt.stats.model = self.model_name
                traces.append(mlt)
            # Compose ComponentStream copy
            cst = ComponentStream(traces=traces,
                                    header={'reference_starttime': next_window_ti,
                                            'reference_sampling_rate': self.ref['sampling_rate'],
                                            'reference_npts': self.ref['npts'],
                                            'aliases': self.aliases},
                                    ref_component=self.ref['component'])
            # Append to queue
            self.queue.append(cst)
            # Update window with an advance for this instrument
            self.window_tracker[site][inst][mod]['ti'] += self.advance_sec
            # Set ready flag to False for this new window (_update_window_tracker will handle re-readying)
            self.window_tracker[site][inst][mod].update({'ready': False})
            # Add to nneww
            nnew += 1
        return nnew
    
    def _reset(self, attr='window_tracker', safety_catch=True):
        if safety_catch:
            if attr in ['window_tracker','queue']:
                answer = input(f'About to delete contents of WindowWyrm.{attr} | Proceed? [Y]/[n]')
            elif attr == 'both':
                answer = input('About to delete contents of WindowWyrm.window_tracker and WindowWyrm.queue | Proceed? [Y]/[n]')
            if answer == 'Y':
                proceed = True
            else:
                proceed = False
        else:
            proceed = True
        if proceed:
            if attr in ['window_tracker', 'both']:
                self.window_tracker = {}
                if safety_catch:
                    print('WindowWyrm.window_tracker reset to empty dictionary')
            if attr in ['queue','both']:
                self.queue = deque()
                if safety_catch:
                    print('WindowWyrm.queue reset to empty deque')

            
    def update_from_seisbench(self, model):
        """
        Helper method for (re)setting the window-defining attributes for this
        WindowWyrm object from a seisbench.models.WaveformModel object:

            self.model_name = model.name
            self.ref['sampling_rate'] = model.sampling_rate
            self.ref['npts'] = model.in_samples
            self.ref['overlap'] = model._annotate_args['overlap'][1]

        """
        if not isinstance(model, sbm.WaveformModel):
            raise TypeError
        elif model.name != 'WaveformModel':
            if model.sampling_rate is not None:
                self.ref.update({'sampling_rate': model.sampling_rate})
            if model.in_samples is not None:
                self.ref.update({'npts': model.in_samples})
            self.ref.update({'overlap': model._annotate_args['overlap'][1]})
            self.model_name = model.name
        else:
            raise TypeError('seisbench.models.WaveformModel base class does not provide the necessary update information')

    def __repr__(self):
        """
        Provide a user-friendly string representation of this WindowWyrm's parameterization
        and state.
        """
        rstr = f'WindowWyrm for model architecture "{self.model_name}"\n'
        rstr += 'Ref.:'
        for _k, _v in self.ref.items():
            if _k in ['overlap', 'npts']:
                rstr += f' {_k}:{_v} samples |'
            elif _k == "sampling_rate":
                rstr += f' {_k}: {_v} sps |'
            elif _k == 'component':
                rstr += f' {_k}:"{_v}" |'
            elif _k == 'threshold':
                rstr += f' {_k}:{_v:.3f}'
        rstr += '\nAliases '
        for _k, _v in self.aliases.items():
            rstr += f' "{_k}":{[__v for __v in _v]}'
        rstr += f'\nIndex: {len(self.window_tracker)} sites |'
        _ni = 0
        for _v in self.window_tracker.values():
            _ni += len(_v) - 1
        types = {}
        for _x in self.queue:
            if type(_x).__name__ not in types.keys():
                types.update({type(_x).__name__: 1})
            else:
                types[type(_x).__name__] += 1
        rstr += f' {_ni} instruments'
        rstr += '\nQueue: '
        for _k, _v in types.items():
            rstr += f'{_v} {_k}(s) | '
        if len(types) > 0:
            rstr = rstr[:-3]
        else:
            rstr += 'Nothing'
            
        return rstr
    
    def __str__(self):
        """
        Provide a string representation of this WindowWyrm's initialization
        """
        rstr = 'wyrm.core.process.WindowWyrm('
        rstr += f'component_aliases={self.aliases}, '
        rstr += f'reference_component="{self.ref["component"]}", '
        rstr += f'reference_completeness_threshold={self.ref["threshold"]}, '
        rstr += f'model_name="{self.model_name}", '
        rstr += f'reference_sampling_rate={self.ref["sampling_rate"]}, '
        rstr += f'reference_npts={self.ref["npts"]}, '
        rstr += f'reference_overlap={self.ref["overlap"]}, '
        rstr += f'max_pulse_size={self.max_pulse_size}, '
        rstr += f'debug={self.debug}'
        for _k, _v in self.options.items():
            if isinstance(_v, str):
                rstr += f', {_k}="{_v}"'
            else:
                rstr += f', {_k}={_v}'
        rstr += ')'
        return rstr

###################################################################################
# METHOD WYRM CLASS DEFINITION - FOR EXECUTING CLASS METHODS IN A PULSED MANNER ###
###################################################################################

class MethodWyrm(Wyrm):
    """
    A submodule for applying a class method with specified key-word arguments to objects
    sourced from an input deque and passed to an output deque (self.queue) following processing.

    NOTE: This submodule assumes that applied class methods apply in-place alterations on data

    """
    def __init__(
        self,
        pclass=ComponentStream,
        pmethod="filter",
        pkwargs={'type': 'bandpass',
                 'freqmin': 1,
                 'freqmax': 45},
        max_pulse_size=10000,
        debug=False,
        ):
        """
        Initialize a MethodWyrm object

        :: INPUTS ::
        :param pclass: [type] class defining object (ComponentStream, MLStream, etc.)
        :param pmethod: [str] name of class method to apply 
                            NOTE: sanity checks are applied to ensure that pmethod is in the
                                attributes and methods associated with pclass
        :param pkwargs: [dict] dictionary of key-word arguments (and positional arguments
                                stated as key-word arguments) for pclass.pmethod(**pkwargs)
                            NOTE: only sanity check applied is that pkwargs is type dict.
                                Users should refer to the documentation of their intended
                                pclass.pmethod() to ensure keys and values are compatable.
        :param max_pulse_size: [int] positive valued maximum number of objects to process
                                during a call of MethodWyrm.pulse()
        :param debug: [bool] should this Wyrm be run in debug mode?

        """

        # Initialize/inherit from Wyrm
        super().__init__(max_pulse_size=max_pulse_size, debug=debug)
        # pclass compatability checks
        if not isinstance(pclass,type):
            raise TypeError('pclass must be a class defining object (type "type")')
        else:
            self.pclass = pclass
        # pmethod compatability checks
        if pmethod not in [func for func in dir(self.pclass) if callable(getattr(self.pclass, func))]:
            raise ValueError(f'pmethod "{pmethod}" is not defined in {self.pclass} properties or methods')
        else:
            self.pmethod = pmethod
        # pkwargs compatability checks
        if isinstance(pkwargs, dict):
            self.pkwargs = pkwargs
        else:
            raise TypeError
        # initialize output queue
        self.queue = deque()

    def pulse(self, x):
        """
        Execute a pulse wherein items are popleft'd off input deque `x`,
        checked if they are type `pclass`, have `pmethod(**pkwargs)` applied,
        and are appended to deque `self.queue`. Items popped off `x` that
        are not type pclass are reappended to `x`.

        Early stopping is triggered if `x` reaches 0 elements or the number of
        iterations equals the initial len(x)

        :: INPUT ::
        :param x: [deque] of [pclass (ideally)]

        :: OUTPUT ::
        :return y: [deque] access to the objects in self.queue
        """
        if not isinstance(x, deque):
            raise TypeError
        qlen = len(x)
        for _i in range(self.max_pulse_size):
            # Early stopping if all items have been assessed
            if _i - 1 > qlen:
                break
            # Early stopping if input deque is exhausted
            if len(x) == 0:
                break
            
            _x = x.popleft()
            if not isinstance(_x, self.pclass):
                x.append(_x)
            else:
                getattr(_x, self.pmethod)(**self.pkwargs);
                # For objects with a stats.processing attribute, append processing info
                if 'stats' in dir(_x):
                    if 'processing' in dir(_x.stats):
                        _x.stats.processing.append(
                            [time.time(),
                             'Wyrm 0.0.0',
                             'MethodWyrm',
                             self.pmethod,
                             f'({self.pkwargs})'])
                self.queue.append(_x)
        y = self.queue
        return y
    
    def __str__(self):
        rstr = f'wyrm.core.process.MethodWyrm(pclass={self.pclass}, '
        rstr += f'pmethod={self.pmethod}, pkwargs={self.pkwargs}, '
        rstr += f'max_pulse_size={self.max_pulse_size}, '
        rstr += f'debug={self.debug})'
        return rstr
    
###################################################################################
# PREDICTION WYRM CLASS DEFINITION - FOR BATCHED PREDICTION IN A PULSED MANNER ####
###################################################################################
    
class PredictionWyrm(Wyrm):
    """
    Conduct ML model predictions on preprocessed data ingested as a deque of
    ComponentStream objects using one or more pretrained model weights. Following
    guidance on model application acceleration from SeisBench, an option to precompile
    models on the target device is included as a default option.

    This Wyrm's pulse() method accepts a deque of preprocessed ComponentStream objects
    and outputs to another deque (self.queue) of MLTrace objects that contain
    windowed predictions, source-metadata, and fold values that are the sum of the
    input data fold vectors
        i.e., data with all 3 data channels has predictions with fold = 3 for all elements, 
              whereas data with both horizontal channels missing produce predictions with 
              fold = 1 for all elements. Consequently data with gaps may have fold values
              ranging \in [0, 3]

    This functionality allows tracking of information density across the prediction stage
    of a processing pipeline.
    """


    def __init__(
        self,
        model=sbm.EQTransformer(),
        weight_names=['pnw',
                      'instance',
                      'stead'],
        devicetype='cpu',
        compiled=True,
        max_pulse_size=1000,
        debug=False):
        """
        Initialize a PredictionWyrm object

        :: INPUTS ::
        :param model: [seisbench.models.WaveformModel] child class object
        :param weight_names: [list-like] of [str] names of pretrained model
                        weights included in the model.list_pretrained() output
                        NOTE: This object holds distinct, in-memory instances of
                            all model-weight combinations, allowing rapid cycling
                            across weights and storage of pre-compiled models
        :param devicetype: [str] name of a device compliant with a torch.device()
                            object and the particular hardware of the system running
                            this instance of Wyrm 
                                (e.g., on Apple M1/2 'mps' becomes an option)
        :param compiled: [bool] should the model(s) be precompiled on initialization
                            using the torch.compile() method?
                        NOTE: This is suggested in the SeisBench documentation as
                            a way to accelerate model application
        :param max_pulse_size: [int] - maximum BATCH SIZE for windowed data
                            to pass to the model(s) for a single call of self.pulse()
        :debug: [bool] should this wyrm be run in debug mode?

        """
        super().__init__(max_pulse_size=max_pulse_size, debug=debug)
        
        # model compatability checks
        if not isinstance(model, sbm.WaveformModel):
            raise TypeError('model must be a seisbench.models.WaveformModel object')
        elif model.name == 'WaveformModel':
            raise TypeError('model must be a child-class of the seisbench.models.WaveformModel class')
        else:
            self.model = model
        
        # Model weight_names compatability checks
        # pretrained_list = model.list_pretrained() # This error catch is now handled with the preload/precopile setp
        if isinstance(weight_names, str):
            weight_names = [weight_names]
        elif isinstance(weight_names, (list, tuple)):
            if not all(isinstance(_n, str) for _n in weight_names):
                raise TypeError('not all listed weight_names are type str')
        # else:
        #     for _n in weight_names:
        #         if _n not in pretrained_list:
        #             raise ValueError(f'weight_name {_n} is not a valid pretrained model weight_name for {model}')
        self.weight_names = weight_names

        # device compatability checks
        if not isinstance(devicetype, str):
            raise TypeError('devicetype must be type str')
        else:
            try:
                device = torch.device(devicetype)
            except RuntimeError:
                raise RuntimeError(f'devicetype {devicetype} is an invalid device string for PyTorch')
            try:
                self.model.to(device)
            except RuntimeError:
                raise RuntimeError(f'device type {devicetype} is unavailable on this installation')
            self.device = device
        
        # Preload/precompile model-weight combinations
        if isinstance(compiled, bool):    
            self.compiled = compiled
        else:
            raise TypeError(f'"compiled" type {type(compiled)} not supported. Must be type bool')

        self.cmods = {}
        for wname in self.weight_names:
            if self.debug:
                print(f'Loading {self.model.name} - {wname}')
            cmod = self.model.from_pretrained(wname)
            if compiled:
                if self.debug:
                    print(f'...pre compiling model on device type "{self.device.type}"')
                cmod = torch.compile(cmod.to(self.device))
            else:
                cmod = cmod.to(self.device)
            self.cmods.update({wname: cmod})
        # Initialize output deque
        self.queue = deque()

    def __str__(self):
        rstr = f'wyrm.core.process.PredictWyrm('
        rstr += f'model=sbm.{self.model.name}, weight_names={self.weight_names}, '
        rstr += f'devicetype={self.device.type}, compiled={self.compiled}, '
        rstr += f'max_pulse_size={self.max_pulse_size}, debug={self.debug})'
        return rstr

    def pulse(self, x):
        """
        Execute a pulse on input deque of ComponentStream objects `x`, predicting
        values for each model-weight-window combination and outputting individual
        predicted value traces as MLTrace objects in the self.queue attribute

        :: INPUT ::
        :param x: [deque] of [wyrm.core.dictstream.ComponentStream] objects
                    objects must be 
        

        TODO: Eventually, have the predictions overwrite the windowed data
              values of the ingested ComponentStream objects so predictions
              largely work as a in-place change
        """
        if not isinstance(x, deque):
            raise TypeError('input "x" must be type deque')
        
        qlen = len(x)
        # Initialize batch collectors for this pulse
        batch_data = []
        batch_fold = []
        batch_meta = []

        for _i in range(self.max_pulse_size):
            if len(x) == 0:
                break
            if _i == qlen:
                break
            else:
                _x = x.popleft()
                if not(isinstance(_x, ComponentStream)):
                    x.append(_x)
                # Check that ComponentStream is ready to split out, copy, and be eliminated
                if _x.ready_to_burn(self.model):
                    # Part out copied data, metadata, and fold objects
                    _data = _x.to_npy_tensor(self.model).copy()
                    _fold = _x.collapse_fold().copy() 
                    _meta = _x.stats.copy()
                    # Attach processing information for split
                    _meta.processing.append([time.time(),
                                             'Wyrm 0.0.0',
                                             'PredictionWyrm',
                                             'split_for_ml',
                                             '<internal>'])
                    # Delete source ComponentStream object to clean up memory
                    del _x
                    # Append copied (meta)data to collectors
                    batch_data.append(_data)
                    batch_fold.append(_fold)
                    batch_meta.append(_meta)
                # TODO: If not ready to burn, kick error
                else:
                    breakpoint()
                    raise ValueError('ComponentStream is not sufficiently preprocessed - suspect an error earlier in the tube')
       
        # IF there are windows to process
        if len(batch_meta) > 0:
            # Concatenate batch_data tensor list into a single tensor
            batch_data = torch.Tensor(np.array(batch_data))
            batch_dst_dict = {_i: DictStream() for _i in range(len(batch_meta))}
            # Iterate across preloaded (and precompiled) models
            for wname, weighted_model in self.cmods.items():
                # Run batch prediction for a given weighted_model weight
                if batch_data.ndim != 3:
                    breakpoint()
                batch_pred = self.run_prediction(weighted_model, batch_data, batch_meta)
                # Reassociate window metadata to predicted values and send MLTraces to queue
                self.batch2dst_dict(wname, batch_pred, batch_fold, batch_meta, batch_dst_dict)
            # Provide access to queue as pulse output
            for _v in batch_dst_dict.values():
                self.queue.append(_v)

        # alias self.queue to output
        y = self.queue
        return y

    def run_prediction(self, weighted_model, batch_data, reshape_output=True):
        """
        Run a prediction on an input batch of windowed data using a specified model on
        self.device. Provides checks that batch_data is on self.device and an option to
        enforce a uniform shape of batch_preds and batch_data.

        :: INPUT ::
        :param weighted_model: [seisbench.models.WaveformModel] initialized model object with
                        pretrained weights loaded (and potentialy precompiled) with
                        which this prediction will be conducted
        :param batch_data: [numpy.ndarray] or [torch.Tensor] data array with scaling
                        appropriate to the input layer of `model` 
        :reshape_output: [bool] if batch_preds has a different shape from batch_data
                        should batch_preds be reshaped to match?
        :: OUTPUT ::
        :return batch_preds: [torch.Tensor] prediction outputs 
        """
        # Ensure input data is a torch.tensor
        if not isinstance(batch_data, (torch.Tensor, np.ndarray)):
            raise TypeError('batch_data must be type torch.Tensor or numpy.ndarray')
        elif isinstance(batch_data, np.ndarray):
            batch_data = torch.Tensor(batch_data)

        # RUN PREDICTION, ensuring data is on self.device
        if batch_data.device.type != self.device.type:
            batch_preds = weighted_model(batch_data.to(self.device))
        else:
            batch_preds = weighted_model(batch_data)

        # If operating on EQTransformer
        nwind = batch_data.shape[0]
        nlbl = len(self.model.labels)
        nsmp = self.model.in_samples
        if self.model.name == 'EQTransformer':
            detached_batch_preds= np.full(shape=(nwind, nlbl, nsmp), fill_value=np.nan, dtype=np.float32)
            for _l, _p in enumerate(batch_preds):
                if _p.device.type != 'cpu': 
                    detached_batch_preds[:, _l, :] = _p.detach().cpu().numpy()
                else:
                    detached_batch_preds[:, _l, :] = _p.detach().numpy()
        elif self.model.name == 'PhaseNet':
            if batch_preds.device.type != 'cpu':
                detached_batch_preds = batch_preds.detach().cpu().numpy() 
            else:
                detached_batch_preds = batch_preds.detach().numpy()
        else:
            raise NotImplementedError(f'model "{self.model.name}" prediction initial unpacking not yet implemented')
        # breakpoint()
        # # Check if output predictions are presented as some list-like of torch.Tensors
        # if isinstance(batch_preds, (tuple, list)):
        #     # If so, convert into a torch.Tensor
        #     if all(isinstance(_p, torch.Tensor) for _p in batch_preds):
        #         batch_preds = torch.concat(batch_preds)
        #     else:
        #         raise TypeError('not all elements of preds is type torch.Tensor')
        # # # If reshaping to batch_data.shape is desired, check if it is required.
        # # if reshape_output and batch_preds.shape != batch_data.shape:
        # #     batch_preds = batch_preds.reshape(batch_data.shape)

        return detached_batch_preds

    def batch2dst_dict(self, weight_name, batch_preds, batch_fold, batch_meta, dst_dict):
        """
        Reassociated batched predictions, batched metadata, and model metadata to generate MLTrace objects
        that are appended to the output deque (self.queue). The following MLTrace ID elements are updated
            component = 1st letter of the model label (e.g., "Detection" from EQTranformer -> "D")
            model = model name
            weight = pretrained weight name
        

        :: INPUTS ::
        :param weight_name: [str] name of the pretrained model weight used
        :param batch_preds: [torch.Tensor] predicted values with expected axis assignments:
                                axis 0: window # - corresponding to the axis 0 values in batch_fold and batch_meta
                                axis 1: label - label assignments from the model architecture used
                                axis 2: values
        :param batch_fold: [list] of [numpy.ndarray] vectors of summed input data fold for each input window
        :param batch_meta: [list] of [wyrm.core.dictstream.ComponentStreamStats] objects corresponding to
                                input data for each prediction window
        :param dst_dict
        
        :: OUTPUT ::
        None

        :: ATTR UPDATE ::
        :attr queue: [deque] MLTrace objects generated as shown below are appended to `queue`
                    for window _i and predicted value label _j
                       mlt = MLTrace(data=batch_pred[_i, _j, :], fold = batch_fold[_i, :], header=batch_meta[_i])

        """

        # # Detach prediction array and convert to numpy
        # if batch_preds.device.type != 'cpu':
        #     batch_preds = batch_preds.detach().cpu().numpy()
        # else:
        #     batch_preds = batch_preds.detach().numpy()
        
        # Reshape sanity check
        if batch_preds.ndim != 3:
            if batch_preds.shape[0] != len(batch_meta):
                batch_preds = batch_preds.reshape((len(batch_meta), -1, self.model.in_samples))

        # TODO - change metadata propagation to take procesing from component stream, but still keep
        # timing and whatnot from reference_streams
        # Iterate across metadata dictionaries
        for _i, _meta in enumerate(batch_meta):
            # Split reference code into components
            # breakpoint()
            n,s,l,c,m,w = _meta.common_id.split('.')
            # Generate new MLTrace header for this set of predictions
            _header = {'starttime': _meta.reference_starttime,
                      'sampling_rate': _meta.reference_sampling_rate,
                      'network': n,
                      'station': s,
                      'location': l,
                      'channel': c,
                      'model': m,
                      'weight': weight_name,
                      'processing': copy.deepcopy(_meta.processing)}
            # Update processing information to timestamp completion of batch prediction
            _header['processing'].append([time.time(),
                                          'Wyrm 0.0.0',
                                          'PredictionWyrm',
                                          'batch2dst_dict',
                                          '<internal>'])
            # Iterate across prediction labels
            for _j, label in enumerate(self.cmods[weight_name].labels):
                # Compose output trace from prediction values, input data fold, and header data
                _mlt = MLTrace(data = batch_preds[_i, _j, :], fold=batch_fold[_i], header=_header)
                # Update component labeling
                _mlt.set_comp(label)
                # Append to window-indexed dictionary of DictStream objects
                if _i not in dst_dict.keys():
                    dst_dict.update({_i, DictStream()})
                dst_dict[_i].__add__(_mlt, key_attr='id')
                # Add mltrace to dsbuffer (subsequent buffering to happen in the next step)
                

