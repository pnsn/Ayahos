"""
:module: wyrm.util.stacking
:auth: Nathan T. Stevens
:email: ntsteven (at) uw.edu
:org: Pacific Northwest Seismic Network
:license: AGPL-3.0

:purpose:
    This module contains subroutines built on numpy that facilitate overlapping
    array stacking and buffering, largely used by the RtPredBuff class, and
    coherence/semblance methods based on code from the ELEP project (Yuan et al., 2023)

:references:
Yuan, C, Ni, Y, Lin, Y, Denolle, M, 2023, Better Together: Ensemble Learning for Earthquake
    Detection and Phase Picking. IEEE Transactions on Geoscience and Remote Sensing, 61,
    doi: 10.1109/TGRS.2023.3320148

"""

import numpy as np
import scipy.ndimage as nd

def shift_trim(array, npts_right, axis=None, fill_value=np.nan, dtype=None, **kwargs):
    """
    Execute a shift of values along a specified axis (default 0-axis),
    omitting (trimming) samples that fall out of the reference frame of the
    array and padding unoccupied sample positions with a specified `fill_value`

    Operation generates a copy of input `array` data, with modifications

    :: INPUTS ::
    :param array: [numpy.ndarray] target array to shift
    :param npts_right: [int] number of samples to shift contents, with a
                positive value indicating a rightward (increasing index)
                shift and a negative value indicating a leftward (decreasing
                index) shift
    :param axis: [int] axis over which to enact the leftshift
    :param fill_value: [scalar] fill_value to insert into empty index positions
                generated by shifting and trimming

    Benchmarking shows that the index-reassignment approach done here
    is more computationally efficient compared to roll_trim_rows below by 
    about a factor of 3

    """
    if not isinstance(array, np.ndarray):
        raise TypeError('input "array" must be type numpy.ndarray')
    
    if not isinstance(npts_right, (int, float)):
        raise TypeError('npts_right must be int-like')
    elif isinstance(npts_right, float):
        npts_right = int(npts_right)
    
    if axis is None:
        pass
    elif axis + 1 > len(array.shape):
        raise IndexError(f'Axis {axis} is outside the dimensionality of array ({array.shape})')

    
    # End processing and return 
    if npts_right == 0:
        return array.copy()
    else:
        if dtype is None:
            tmp_array = np.full(shape=array.shape, fill_value=fill_value, dtype=array.dtype, **kwargs)
        else:
            tmp_array = np.full(shape=array.shape, fill_value=fill_value, **kwargs)

        # If ellipsis use case
        if axis is None:
            # rightshift
            if npts_right > 0:
                tmp_array[npts_right:, ...] = array.copy()[:-npts_right, ...]
            # leftshift
            elif npts_right < 0:
                tmp_array[:npts_right, ...] = array.copy()[-npts_right:, ...]
            
        elif axis + 1 == array.ndim or axis == -1:
            # rightshift
            if npts_right > 0:
                tmp_array[..., npts_right:] = array.copy()[..., :-npts_right]
            # leftshift
            elif npts_right < 0:
                tmp_array[..., :npts_right] = array.copy()[..., -npts_right:]
        # hack-ey use of 'eval' to do dynamic indexing
        elif axis + 1 < array.ndim:
            # Start eval strings for indexing
            tmp_index = '['
            array_index = '['
            for _i in range(array.ndim):
                # If this is not the target axis, insert :
                if _i != axis:
                    tmp_index += ':'
                    array_index += ':'
                # If this is the target axis
                else:
                    # append rightshift slice
                    if npts_right > 0:
                        tmp_index += 'npts_right:'
                        array_index += ':-npts_right'
                    # append lefshift slice
                    elif npts_right < 0:
                        tmp_index += ':-npts_right'
                        array_index == 'npts_right:'
                # If this isn't the final axis, append a comma
                if _i + 1 < len(array.shape):
                    tmp_index += ', '
                    array_index += ', '
                # Otherwise, close out eval str
                else:
                    tmp_index += ']'
                    array_index += ']'
            eval(f'tmp_array{tmp_index} = array.copy(){array_index}')
    
    return tmp_array

def sum2(arr):
    """"""
    return sum(arr)

def semblance(pstack, fstack=None, order=2, semb_npts=101, method='np', eta=1e-9, weighted=True):
    """
    Calculate coherence via semblance analysis

    Based on ELEP's ensemble_coherence method
    Yuan et al. (2023) and references therein

    Semblance:
    semb(t) = MAX[m](P(m,t))*C[m,t](P(m,t))**order

    Coherence:
    C(P(t)) = sum[ti, tj]{sum[m]{P(m,t)**2}}/
                M*sum[ti,tj]{sum[m]{P(m,t)**2}}

    With M = the number of entries in "m"

    :: INPUTS ::
    :param pstack: [numpy.ndarray] containing stacked, synchronous windowed
                    prediction outputs from "m" model-weight combinations
                    for a given prediction label "l" time-series with sample
                    points "t".
                    This version requires the following axis assignment
                        pstack [m, l, t]
                            or
                        pstack [m, t]
                        when run in method = 'nd'
                    or generally
                        pstack [m, ..., t]
                        when run in method = 'np'
 
                    NOTE
                    If mixing model types (i.e., EQTransformer & PhaseNet)
                    care is required to make sure l-axis inputs are compatable
                    i.e., use prediction labels 1 (P-detection) and 2 (S-detection)
                        but likely omit label 0 (Detection for EqT, Noies for PhaseNet)

    :param order: [int] exponent to apply to the semblance term
    :param semb_npts: [int] number of samples in the window [ti, tj]
            NOTE: computational cost scales roughly linearly with semb_npts
    :param method: [str] method flag for the algorithm to produce rolling windows
                of stack along the t_axis.
            Supported:
                "nd" - use SciPy's ndimage.generic_filter() method (after Yuan et al., 2023)
                        Slightly less computationally efficient, but provides estimates
                        at the edges of arrays.
                "np" - use NumPy's lib.stride_tricks.sliding_window_view() method
                        Slightly more computationally efficient, but blinds the
                        first and last semb_npts/2 points in each row vector of semb_array
                        (blind = np.nan)
    :param eta: [float] very small floating point number added to the denominator of
                C(P(t)) to prevent division by 0. Default is 1e-9. 
    :param weighted: [bool] should the MAX weighting term be applied?
                        True - weighting is via MAX (weighted semblance metric)
                        False - weighting is uniformly 1 (original semblance metric)
                        
    :: OUTPUT ::
    :return semb_array: [numpy.ndarray] containing semblance time-series with dimensionality
                semb_array [l, t] for input pstack [m, l, t]
                    or
                semb_array [t] for input pstack [m, t]
    """
    # TODO - compatability checks
    if eta < 0:
        eta = np.abs(eta)

    if weighted:
        # Calculate max weighting for each P(l,t)
        wt = np.max(pstack,axis=0)
    else:
        wt = np.ones(shape=pstack.shape[1:])

    if method == 'np':
        # Preallocate output array
        semb_array = np.full(shape=pstack.shape[1:],
                            fill_value=np.nan,
                            dtype=pstack.dtype)
        # Get first write location
        first_t = semb_npts//2
        # Generate views
        views = np.lib.stride_tricks.sliding_window_view(pstack, semb_npts, axis=-1)
        # Calculate numerator
        num = np.sum(np.sum(views, axis=0)**2, axis=-1)
        # Calculate denominator
        den = pstack.shape[0]*np.sum(views**2, axis=(0,-1))
        semb_array[..., first_t: first_t + views.shape[-2]] = (num/(den+eta))**order
        semb_array *= wt

    if method == 'nd':
        # Calculate internal sum/square combinations
        sq_sums = np.sum(pstack, axis=0)**2
        sum_sqs = np.sum(pstack**2, axis=0)
        # Iterate across labels
        if pstack.ndim == 3:
            for _l in range(pstack.shape[1]):
                # Apply 
                num = nd.generic_filter(sq_sums[_l, :], sum, semb_npts, mode='constant')
                den = pstack.shape[0]*nd.generic_filter(sum_sqs[_l, :], sum, semb_npts, mode='constant')
                semb_array[_l, :] = wt[_l, :]*(num/(den+eta))**order
        # Handle case where label axis has been squeezed
        elif pstack.ndim == 2:
            num = nd.generic_filter(sq_sums, sum, semb_npts, mode='constant')
            den = pstack.shape[0]*nd.generic_filter(sum_sqs, sum, semb_npts, mode='constant')
            semb_array = wt*(num/(den+eta))**order


    return semb_array

def fold_weighted_semblance(
        pstack,
        fstack=None,
        semb_order=2,
        max_scaled=True,
        semb_npts=101,
        method='np',
        eta=1e-9):
    """
    Use a matching dimensionality fold array for predictions to calculate a 


             sum(t=ti - dt to ti + dt){ sum(m=0 to M-1) {P_m(t)*f_m(t)}**2}
    Sw(ti) = --------------------------------------------------------------
             sum(t=ti - dt to ti + dt){(sum(m=0 to M-1){P_m(t)**2} sum(m=0 to M-1){f_m(t)**2}}

    https://reproducibility.org/RSF/book/tccs/vscan/paper_html/node4.html
    """
    # Parse fold weighting array
    if fstack is None:
        fstack = np.ones(pstack.shape, dtype=pstack.dtype)
        ftype = 'ones'
    else:
        ftype = 'data'
    if pstack.shape != fstack.shape:
        raise ValueError(f'pstack and fstack shape do not match')
    # Get maximum scalar vector
    if max_scaled:
        max_scalar = np.max(pstack, axis=0)
    else:
        max_scalar = np.ones(pstack.shape[1])

    if method in ['numpy', 'np']:
        semb_array = np.full(shape=pstack.shape[1:],
                             fill_value=np.nan,
                             dtype=pstack.dtype)
        first_t = semb_npts//2
        # Generate sliding window views of data and fold
        pviews = np.lib.stride_tricks.sliding_window_view(pstack, semb_npts, axis=-1)
        fviews = np.lib.stride_tricks.sliding_window_view(fstack, semb_npts, axis=-1)

M = pstack.shape[0]
num = np.sum(np.sum(pviews*fviews, axis=0)**2, axis=-1)
den = np.sum(np.sum(pviews, axis=0)**2 * np.sum(fviews, axis=0)**2, axis=-1)
semb_array[..., first_t: first_t + pviews.shape[-2]] = num / (den + eta)
        
c2 = np.where(semb_array > 0, semb_array**2, 0)
c2 *=max_scalar



def generate_example_semblance_data(nlabels=1,mmodels=30):
    # Create holder
    pstack = np.zeros(shape=(mmodels, nlabels, 3000))
    # Create example vector
    vect = np.zeros(3000)
    # Create a bell-curvey probability distribution
    vect[1000:1009] = np.array([0.01,0.03,0.1,0.3,1,0.3,0.1,0.03,0.01])
    # Iterate across model and label indices
    for _m in range(mmodels):
        for _l in range(nlabels):
            # Perturb example vector with roll and add to pstack
            pstack[_m, _l, :] = np.roll(vect.copy(), (_m+_l)%4 + _l)
    return pstack